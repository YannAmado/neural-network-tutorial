{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19ba97a",
   "metadata": {},
   "source": [
    "This notebooks improves from the previous version by changing most of the fors into numpy matrix multiplication on:<br>\n",
    "-feedForward \n",
    "-backProp\n",
    "\n",
    "Advantages:<br>\n",
    "-It's a lot faster<br>\n",
    "\n",
    "The focus of the previous notebooks were understanding the operations involved in neural networks, focusing more on the code and mechanisms involved than on the math. On this notebook, we utilize matrix operations to make it faster and more concise, and if any of the operations seem strange, just return to the previous notebook and compare the code, the math is the same at the end. <br>\n",
    "This notebook was made mostly following Michael Nielsen's book http://neuralnetworksanddeeplearning.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14a426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "from keras.datasets import mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67dbe35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    The main object we're going to use accross this notebook\n",
    "    It's a neural network that takes as input a list of \n",
    "    layers nodes\n",
    "    \n",
    "    Ex: [2, 3, 1] is a 3 layer network, with 2 neurons of input, 3 neurons \n",
    "    in the hidden layer and 1 for the output layer\n",
    "    \n",
    "    Supposedly it can take more than just 3 layers but I didnt test it\n",
    "    \n",
    "    It initializes an object with the proper weights, biases, activations and z\n",
    "    based on the layers list. It also has the layers list and the number of layers\n",
    "    \n",
    "    The weights and biases initialized following a Gaussian with standard deviation 1/sqrt(n_in)\n",
    "    with n_in = number of weights into the neuron\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: list):        \n",
    "        np.random.seed(42)        \n",
    "        b = []\n",
    "        w = []\n",
    "        a = []\n",
    "        z = []\n",
    "        for l in range(0, len(layers)):\n",
    "            # skipping one layer for the weights and biases\n",
    "            if (l+1) < len(layers):\n",
    "                b.append(np.random.normal(loc=0, scale=1,size=layers[l+1]))\n",
    "                wScale = 1/np.sqrt(layers[l])\n",
    "                w.append(np.random.normal(loc=0,scale=wScale,size=[layers[l],layers[l+1]]))\n",
    "                #print(w[l])\n",
    "            a.append(np.zeros(layers[l]))\n",
    "            z.append(np.zeros(layers[l]))\n",
    "        # b[i][j] -> \"i\" is which layer, \"j\" which neuron\n",
    "        # w[i][j][k] -> \"i\" is which layer, \"j\" which neuron of the first layer, \"k\" which neuron of the second layer\n",
    "        self.b = b\n",
    "        self.w = w\n",
    "        self.a = a\n",
    "        self.z = z\n",
    "        self.nLayers = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "    @staticmethod\n",
    "    def copy(net):\n",
    "        copiedNet = Network([784,30,10])\n",
    "        copiedNet.a = np.copy(net.a)\n",
    "        copiedNet.z = np.copy(net.z)\n",
    "        for l in range(2):\n",
    "            copiedNet.w[l] = np.copy(net.w[l])\n",
    "            copiedNet.b[l] = np.copy(net.b[l])\n",
    "        return copiedNet\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "378ffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n: float):\n",
    "    return 1.0/(1.0+np.exp(-n))\n",
    "\n",
    "def sigmoid_derivative(n: float):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(n)*(1-sigmoid(n))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ea48a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(net: Network) -> Network:\n",
    "    \"\"\"\n",
    "    Feedforwading the activations to the next layer\n",
    "    \n",
    "    It will take as input the network already with the input image as the activation \n",
    "    on the first layer and then feedforward to the next layrse\n",
    "    \n",
    "    It returns the network with all the activations set\n",
    "    \"\"\"\n",
    "    \n",
    "    for l in range(0, net.nLayers-1):\n",
    "        net.z[l+1] = np.dot(np.transpose(net.a[l]), net.w[l]) + net.b[l]\n",
    "        net.a[l+1] = sigmoid(net.z[l+1])\n",
    "            \n",
    "    return net\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10120406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setInput(net: Network, MNISTnumber):\n",
    "    \"\"\"\n",
    "    Inputs the MNIST number into the network, since the number is a 28x28 matrix, \n",
    "    we transform it into a 784 array\n",
    "    \n",
    "    We also scale the pixels as to be between 0 and 1 for the sigmoid function \n",
    "    instead of 0 and 255\n",
    "    \n",
    "    Returns the network with the proper activations on all layers since it pass \n",
    "    through the feedforward step\n",
    "    \"\"\"\n",
    "    numberArr = np.asarray(MNISTnumber).flatten()\n",
    "    # scaling the array so that the range is between 0 and 1\n",
    "    numberArr = np.interp(numberArr, (numberArr.min(), numberArr.max()), (0, 1))\n",
    "    net.z[0] = numberArr\n",
    "    net.a[0] = numberArr\n",
    "    net = feedForward(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d026ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNetwork(net: Network, test_X, test_y, nTests: int):\n",
    "    \"\"\"\n",
    "    A function to test our network\n",
    "    \n",
    "    It returns the overall accuracy and the numbers our network guessed\n",
    "    \"\"\"\n",
    "    \n",
    "    correctOutput = 0\n",
    "    X = test_X[:nTests]\n",
    "    y = test_y[:nTests]\n",
    "    outputs = np.zeros(10)\n",
    "    for i in range(nTests):\n",
    "        net = setInput(net, X[i])\n",
    "        networkOutput = np.argmax(net.a[-1])\n",
    "        outputs[networkOutput] += 1\n",
    "        #print(f\"number: {y[i]}, networkOutput: {networkOutput}, activations: {net.a[-1]}\")\n",
    "        if y[i] == networkOutput:\n",
    "            correctOutput += 1\n",
    "    acc = correctOutput/nTests\n",
    "    return acc, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d4f2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(train_X, train_y, test_X, test_y, batchSize: int, learningRates: list, epochs: int, lamb):\n",
    "    \"\"\"\n",
    "    A function to perform a gridSearch in order to find the best learningRates\n",
    "\n",
    "    It takes as input the network, the training images of MNIST, the training labels,\n",
    "    the test images, the test labels, the batchSize for SGD,\n",
    "    a list of learningRates as to find the best inside the list\n",
    "    the number of epochs to perform SGD\n",
    "    \n",
    "    \n",
    "    It returns the best network accross all learning rates list\n",
    "    \"\"\"\n",
    "    bestAcc = 0\n",
    "    for eta in learningRates:\n",
    "        # resetting the network\n",
    "        net = Network([784,30,10])\n",
    "        net = SGD(net, train_X, train_y, batchSize=batchSize, nEpochs=epochs, learningRate=eta, lamb=lamb)\n",
    "        acc, outputs = testNetwork(net, test_X, test_y, batchSize) \n",
    "        if acc > bestAcc:\n",
    "            bestNet = net\n",
    "            bestAcc = acc\n",
    "    return bestNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491dbfe",
   "metadata": {},
   "source": [
    "The list below is all equations that were used to compute the erros and then propagate through the network:\n",
    "\n",
    "To calculate the error on the last layer: \n",
    "$$\\delta^L = (a^L - y)\\odot \\sigma'(z^L)$$\n",
    "\n",
    "To calculate the error on the other layers:\n",
    "$$\\delta^l = ((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)$$\n",
    "\n",
    "To repass the error to the bias: \n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$$\n",
    "\n",
    "To repass the error to the weights:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k\\delta^l_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b397ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(net: Network, y) -> Network:\n",
    "    \"\"\"\n",
    "    The backpropagation step: first we calculate the error on the last layer, \n",
    "    then we pass to the previous layers all the while applying the error \n",
    "    to the weights and biases. Here we used Cross-entropy as our cost function\n",
    "    \n",
    "    Example on a 3 layer network: We calculate the error on the last layer, \n",
    "    apply it to the last layer's weights and biases, and then calculate the \n",
    "    error on the next layer, propagate to the weights and biases and it's done\n",
    "    \n",
    "    It takes as input the network and the label of the number the network was activated on\n",
    "    \n",
    "    It returns the modifications to the weights and biases (nablaW and nablaB) \n",
    "    the network should have\n",
    "    \"\"\"\n",
    "    layers = net.layers\n",
    "    nablaB = [np.zeros(i.shape) for i in net.b]\n",
    "    nablaW = [np.zeros(i.shape) for i in net.w]\n",
    "    delta = np.array(net.a[-1]) - 0\n",
    "    delta[y] = net.a[-1][y] - 1\n",
    "    for l in range(net.nLayers-1, 0, -1):\n",
    "        #nablaB and nablaW have -1 because they only have 2 layers instead of 3\n",
    "        nablaB[l-1] = delta\n",
    "                \n",
    "        nablaW[l-1] = np.outer(net.a[l-1], delta)        \n",
    "        # finding the error one layer behind\n",
    "        # in the book it needs a transpose because its weight[layer][receivingNeuron][givingNeuron]\n",
    "        # but my implementation uses weight[layer][givingNeuron][receivingNeuron] so it's not necessary\n",
    "        delta = (np.dot(net.w[l-1], delta))*sigmoid_derivative(net.z[l-1])\n",
    "        \n",
    "    return nablaB, nablaW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8536b0c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(net: Network, X: list, y: list, batchSize: int, nEpochs: int, learningRate, lamb, earlyStop=False) -> Network:\n",
    "    \"\"\"\n",
    "    Implementation of Stochastic Gradient Descent\n",
    "    \n",
    "    It takes as input the network, the MNIST dataset, the MNIST labels of the dataset, \n",
    "    the size of the batch to do gradient descent, the number of epochs it should run,\n",
    "    the learning rate eta (I found the best eta to be in the order of 1s)\n",
    "    and the regularization term lambda\n",
    "    \n",
    "    It returns a trained network\n",
    "    \"\"\"\n",
    "    bestAcc = 0\n",
    "    bestEpoch = 0\n",
    "    earlyNet = net\n",
    "    eta = learningRate\n",
    "    etaChangeEpoch = 0\n",
    "    for epoch in range(nEpochs):\n",
    "        batch = rd.sample(range(len(X)), batchSize)\n",
    "        nablaB = [np.zeros(i.shape) for i in net.b]\n",
    "        nablaW = [np.zeros(i.shape) for i in net.w]\n",
    "        for i in batch:\n",
    "            net = setInput(net, X[i])\n",
    "            # finding what should be modified based on this particular example\n",
    "            deltaNablaB, deltaNablaW = backProp(net, y[i])\n",
    "            # passing this modifications to our overall modifications matrices\n",
    "            for l in range(net.nLayers-1):\n",
    "                nablaB[l] += deltaNablaB[l]\n",
    "                nablaW[l] += deltaNablaW[l]\n",
    "        \n",
    "        # applying the changes to our network\n",
    "        for l in range(net.nLayers-1):\n",
    "            net.b[l] = net.b[l] - eta * (nablaB[l]/batchSize) \n",
    "            net.w[l] = net.w[l] - eta * (nablaW[l]/batchSize) - eta * (lamb/batchSize) *  net.w[l]\n",
    "        acc, outputs = testNetwork(net, X, y, nTests=batchSize)\n",
    "        if acc > bestAcc:\n",
    "            bestAcc = acc\n",
    "            bestEpoch = epoch\n",
    "            earlyNet = Network.copy(net)\n",
    "            etaChangeEpoch = epoch\n",
    "        if epoch%10 == 0:\n",
    "            print(f'learningRate: {learningRate} epochs: {epoch} acc: {acc}, outputs: {outputs}')\n",
    "    print(f'best acc: {bestAcc} on epoch: {bestEpoch}')\n",
    "    if earlyStop:\n",
    "        return earlyNet\n",
    "    return net\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38ebf075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5918a0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningRate: 2 epochs: 0 acc: 0.06, outputs: [  0.   0. 100.   0.   0.   0.   0.   0.   0.   0.]\n",
      "learningRate: 2 epochs: 100 acc: 0.92, outputs: [14. 13.  6. 12. 11.  4. 12. 11.  8.  9.]\n",
      "learningRate: 2 epochs: 200 acc: 0.94, outputs: [14. 13.  5. 13. 11.  5. 11. 12.  8.  8.]\n",
      "best acc: 0.97 on epoch: 279\n"
     ]
    }
   ],
   "source": [
    "net = gridSearch(train_X, train_y, test_X, test_y, batchSize=100, learningRates=[2], epochs=200, lamb=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2f632dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9189,\n",
       " array([1001., 1166.,  956., 1046.,  991.,  893.,  997.,  971.,  969.,\n",
       "        1010.]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testNetwork(net, test_X, test_y, len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5602a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing our network in action\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pick a sample to plot\n",
    "sample = 50099\n",
    "image = train_X[sample]\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "net = setInput(net, train_X[sample])\n",
    "networkOutput = np.argmax(net.a[-1])\n",
    "networkOutput"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
