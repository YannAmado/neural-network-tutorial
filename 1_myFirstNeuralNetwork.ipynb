{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d629ab",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a intuitive guide made for those who are just now getting into neural networks, the objective is to go step by step into how to build a neural network, making sure each and every step is fully understandable. We will be following the book made by Michael Nielsen (link to the book: http://neuralnetworksanddeeplearning.com/index.html), and will try to make the code easier to understand, since a few steps might get lost when going from the book into the code\n",
    "\n",
    "We will be working with the Hello World of neural Networks, The MNIST Dataset, which is a collection of handwritten digits as well as the respective labels (1, 2, 3...). The network will try to correctly classify the digits, and the accuracy will be displayed and used as a measure to how well the network did.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7433b43",
   "metadata": {},
   "source": [
    "# Necessary libraries:\n",
    "\n",
    "If you dont have any of the libraries necessary, just run the following commands on your windows/linux terminal:\n",
    "* Numpy: !pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f871516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8098c0",
   "metadata": {},
   "source": [
    "# First Step: Creating the network\n",
    "\n",
    "In order to create a network, we will be taking advantage of python's classes.\n",
    "\n",
    "If you dont know what a class is, I suggest you take a look at the following link: https://www.w3schools.com/python/python_classes.asp\n",
    "\n",
    "## Why use classes?\n",
    "\n",
    "Classes are just an easy form to make the code easier to use, aggregating meaningful attributes into a single object.\n",
    "In this manner, we are able to more easily track each attribute of the network.\n",
    "\n",
    "\n",
    "### Explaining necessary attributes:\n",
    "\n",
    "In order to create a neural network, a few attributes will be necessary:\n",
    "\n",
    "* number of inputs (nInputs): How many input neurons will the network create. For this simple NN (Neural Network), each pixel will represent an input neuron. The MNIST images have each 28x28 pixels, thus, we will have 28x28 input neurons\n",
    "* number of HiddenLayers (nHiddenLayers): How many hiddend layers should the network have. For a simple NN, only 1 hidden layer will suffice, but for more complex projects, multiple hidden layers may be used (a network with multiple hidden layers is called a *deep neural network* ). The purpose of the hidden layers is to try and \"catch\" patterns of the image, with each subsequent layer bulding upon the last; so for example, a network with 3 hidden layers could work like the following: the first hl (hidden layer) catches the edges, the second turn the edges into shapes (like circles, squares, triangles), and the third turn the shapes into more complex shapes (like eyes, mouth, etc).\n",
    "* number of Neurons per Hidden Layer: How many neurons will each hidden layer have. This is kind of an arbitrary choice, for this network we will be using 30 hidden neurons, simply because this is what Michael Nielsen did in his book. \n",
    "* number of output neurons (nOutputs): How many output Neurons should the network have. It's a good idea for each outcome to have a neuron specific to it, and since we are trying to classify 10 different digits (0 to 9), we will be using 10 output neurons\n",
    "\n",
    "Aside from those attributes which the user will give to the network, other attributes will be made based on those initial ones.\n",
    "### Weights: \n",
    "The weights between two neurons from different layers. For simplicity, we are using numpy to build the matrices with each index assuming a random value between -5 and 5. \n",
    "The matrix we need to build is a 3D matrix. 3D matrix might be scary at first, but our code was structured to have an intuitive manner of accessing each component:\n",
    "* The first index is which layer the \"weight layer\" we are dealing with. A \"weight layer\" is simply the layer of weights between two neurons\n",
    "* The second index is from what neuron the weight is exiting from\n",
    "* The third index is to what neuron the weight is arriving\n",
    "\n",
    "### Bias: \n",
    "For the bias, a 2D matrix will suffice. Like with the weights, the biases will also be started randomly\n",
    "* The first index tells which layer the bias belongs to\n",
    "* The second index tells which neuron the bias belongs to\n",
    "\n",
    "The following code will create the class with all atributes necessary\n",
    "\n",
    "### Activation:\n",
    "For the activation of the neurons, a 2D matrix will also suffice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e2eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    The main object we're going to use accross this notebook\n",
    "    It's a neural network that takes as input a list of \n",
    "    layers nodes\n",
    "    \n",
    "    Ex: [2, 3, 1] is a 3 layer network, with 2 neurons of input, 3 neurons \n",
    "    in the hidden layer and 1 for the output layer\n",
    "    \n",
    "    Supposedly it can take more than just 3 layers but I didnt test it\n",
    "    \n",
    "    It initializes an object with the proper weights, biases, activations and z\n",
    "    based on the layers list. It also has the layers list and the number of layers\n",
    "    \n",
    "    The weights and biases initialized following a Gaussian of mean 1\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: list):        \n",
    "        np.random.seed(42)        \n",
    "        b = []\n",
    "        w = []\n",
    "        a = []\n",
    "        z = []\n",
    "        for l in range(0, len(layers)):\n",
    "            # skipping one layer for the weights and biases\n",
    "            if (l+1) < len(layers):\n",
    "                b.append(np.random.normal(loc=0, scale=1,size=layers[l+1]))\n",
    "                w.append(np.random.normal(loc=0,scale=1,size=[layers[l],layers[l+1]]))\n",
    "            a.append(np.zeros(layers[l]))\n",
    "            z.append(np.zeros(layers[l]))\n",
    "    \n",
    "        # b[i][j] -> \"i\" is which layer, \"j\" which neuron\n",
    "        # w[i][j][k] -> \"i\" is which layer, \"j\" which neuron of the first layer, \"k\" which neuron of the second layer\n",
    "        self.b = b\n",
    "        self.w = w\n",
    "        self.a = a\n",
    "        self.z = z\n",
    "        self.nLayers = len(layers)\n",
    "        self.layers = layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef50af5",
   "metadata": {},
   "source": [
    "# The sigmoid function\n",
    "In order to our activation neurons on the hidden layers to not explode (meaning, not have an exagerated value, which in turn would mean an exagerated importance); we implement the sigmoid function. It has a few nice traits that are of use to us and is simple enough that can be implement easily, the book has more details about our choice (chapter 1 in the section sigmoid neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d222db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(number):\n",
    "    sigNumber = 1/(1 + np.exp(-number))\n",
    "    return sigNumber  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873b791",
   "metadata": {},
   "source": [
    "# Feeding Forward\n",
    "Now, for the bread and butter of neural networks, the feeding forward step. Feeding forward is one of the most fundamental steps of neural networks, and consists of passing information from previous layers to later layers. Combining weights, activations and biases, our neurons will be able to try and find patterns and relative importances on the input, with each passing layer finding (hopefuly) increasingly complex patterns.\n",
    "In pratical terms, the logic is fairly simple: Starting from the input layer, all neurons from the previous layer will pass information to ALL neurons of the next layer, making the iconic shape that represents neural networks. \n",
    "If you dont understand (or wanta a nice visual explanation), I suggest this video of 3blue1brown explaining how it works: https://youtu.be/aircAruvnKk?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=274\n",
    "\n",
    "## Explaining the logic of our code:\n",
    "The logic is as follow:\n",
    "* Starting from the input layer, find the number of neurons of the giving layer and of the receiving (next) layer\n",
    "* Then, loop through every neuron of the receiving layer in the following manner: First, get the bias by inputing the layer and position of the  neuron. Second, loop through every neuron of the previous layers, combining the activation with its respective weight, and summing the results.\n",
    "* Finally, change layers, with the receiving layer now being the giving layer, and the receiving layer being the next one after the current layer.\n",
    "* Repeat until there are no more receiving layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa38901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(net: Network) -> Network:\n",
    "    \"\"\"\n",
    "    Feedforwading the activations to the next layer\n",
    "    \n",
    "    It will take as input the network already with the input image as the activation \n",
    "    on the first layer and then feedforward to the next layrse\n",
    "    \n",
    "    It returns the network with all the activations set\n",
    "    \"\"\"\n",
    "    \n",
    "    # resetting the activations as to not take any info from the activation of\n",
    "    # the previous number while maintanin the first activation\n",
    "    for i in range(1, net.nLayers):\n",
    "        net.z[i] = np.zeros(net.layers[i])\n",
    "        net.a[i] = np.zeros(net.layers[i])\n",
    "    for l in range(0, net.nLayers-1):\n",
    "        for receivingNeuron in range(net.layers[l+1]):\n",
    "            for givingNeuron in range(net.layers[l]):\n",
    "                net.z[l+1][receivingNeuron] += net.a[l][givingNeuron] * net.w[l][givingNeuron][receivingNeuron]\n",
    "            net.z[l+1][receivingNeuron] += net.b[l][receivingNeuron]\n",
    "            net.a[l+1][receivingNeuron] = sigmoid(net.z[l+1][receivingNeuron])\n",
    "\n",
    "            \n",
    "    return net\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa91b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setInput(net: Network, MNISTnumber):\n",
    "    \"\"\"\n",
    "    Inputs the MNIST number into the network, since the number is a 28x28 matrix, \n",
    "    we transform it into a 784 array\n",
    "    \n",
    "    We also scale the pixels as to be between 0 and 1 for the sigmoid function \n",
    "    instead of 0 and 255\n",
    "    \n",
    "    Returns the network with the proper activations on all layers since it pass \n",
    "    through the feedforward step\n",
    "    \"\"\"\n",
    "    numberArr = np.asarray(MNISTnumber).flatten()\n",
    "    # scaling the array so that the range is between 0 and 1\n",
    "    numberArr = np.interp(numberArr, (numberArr.min(), numberArr.max()), (0, 1))\n",
    "    for i in range(net.layers[0]):\n",
    "        net.z[0][i] = numberArr[i]\n",
    "        net.a[0][i] = numberArr[i]\n",
    "    net = feedForward(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f2b07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(network: Network):\n",
    "    maxIndex = np.argmax(network.a[-1])\n",
    "    return maxIndex, network.a[-1][maxIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fa6d6",
   "metadata": {},
   "source": [
    "# Getting the MNIST Dataset\n",
    "There are several ways of getting the MNIST Dataset, I used the keras library just because it seemed the easiest, but feel free to choose what you prefer.\n",
    "If you dont really want to look for an alternative way, use the following commands to install the keras library (keep in mid that the tensorflow library needed is fairly large):\n",
    "* !pip install tensorflow\n",
    "* !pip install keras.\n",
    "\n",
    "Using keras, our data will already be split, between training and testing. If you did it any other way, just make sure that roughly 25% of the images goes to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1345fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487baa92",
   "metadata": {},
   "source": [
    "# Passing images through the network\n",
    "Note that it may take a few minutes, depending on your hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3755c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "net = Network([784,30,10])\n",
    "hits = 0\n",
    "misses = 0\n",
    "for currentImage in range(10):\n",
    "    #passing the inputs to our network\n",
    "    net = setInput(net, trainX[currentImage])\n",
    "    net = feedForward(net)\n",
    "    output, certainty  = classify(net) \n",
    "    if output == trainY[currentImage]:\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "    #print(f\"Network Classified as : {numberNetworkThinks}, certainty: {certainty}, real number: {trainY[currentImage]}\")\n",
    "acc = hits/misses\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47a97c",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "You know have a fully functional, guess generator. Since our network does not have any learning, it's output is no better than a random a guess; in turn, ou accuracy reflects that line of thought (roughly, our network guessed 10% of tests righ). In the next notebook, we will implement learning, and our network will finaly be of use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
